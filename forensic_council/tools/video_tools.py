"""
Video Forensic Tools
====================

Real forensic tool handlers for video analysis.
Implements optical flow analysis, frame extraction, frame consistency analysis,
and face swap detection.
"""

from __future__ import annotations

import hashlib
import os
import tempfile
from dataclasses import dataclass
from datetime import datetime, timezone
from typing import Any, Optional
from uuid import uuid4

import cv2
import numpy as np

from core.evidence import ArtifactType, EvidenceArtifact
from core.exceptions import ToolUnavailableError


@dataclass
class FrameInconsistency:
    """Frame inconsistency detected in video."""
    frame_pair: tuple[int, int]
    diff_score: float
    type: str
    
    def to_dict(self) -> dict[str, Any]:
        return {
            "frame_pair": list(self.frame_pair),
            "diff_score": self.diff_score,
            "type": self.type,
        }


async def optical_flow_analyze(
    artifact: EvidenceArtifact,
    flow_threshold: float = 5.0,
) -> dict[str, Any]:
    """
    Perform optical flow analysis on video.
    
    Uses OpenCV Farneback optical flow on full video,
    computes per-frame motion vectors, and flags statistical outliers.
    
    Args:
        artifact: The evidence artifact to analyze
        flow_threshold: Threshold for flagging motion anomalies
    
    Returns:
        Dictionary containing:
        - anomaly_heatmap_artifact: Derivative artifact with motion heatmap
        - flagged_frames: List of frame numbers with anomalies
        - motion_stats: Motion statistics across video
    
    Raises:
        ToolUnavailableError: If file cannot be processed
    """
    try:
        video_path = artifact.file_path
        if not os.path.exists(video_path):
            raise ToolUnavailableError(f"File not found: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ToolUnavailableError(f"Cannot open video: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        # Read first frame
        ret, prev_frame = cap.read()
        if not ret:
            raise ToolUnavailableError("Cannot read video frames")
        
        prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)
        
        # Accumulate flow magnitudes
        flow_magnitudes = []
        frame_idx = 0
        flagged_frames = []
        
        # Create heatmap accumulator
        heatmap_accumulator = np.zeros((height, width), dtype=np.float32)
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
            
            frame_idx += 1
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Compute optical flow using Farneback method
            flow = cv2.calcOpticalFlowFarneback(
                prev_gray, gray,
                None,
                pyr_scale=0.5,
                levels=3,
                winsize=15,
                iterations=3,
                poly_n=5,
                poly_sigma=1.2,
                flags=0
            )
            
            # Compute flow magnitude
            magnitude = np.sqrt(flow[..., 0]**2 + flow[..., 1]**2)
            mean_magnitude = np.mean(magnitude)
            flow_magnitudes.append(mean_magnitude)
            
            # Accumulate for heatmap
            heatmap_accumulator += magnitude
            
            prev_gray = gray
        
        cap.release()
        
        # Analyze flow magnitudes for anomalies
        if len(flow_magnitudes) > 0:
            flow_array = np.array(flow_magnitudes)
            mean_flow = np.mean(flow_array)
            std_flow = np.std(flow_array)
            
            # Flag frames with unusual motion
            for i, mag in enumerate(flow_magnitudes):
                if std_flow > 0:
                    z_score = abs(mag - mean_flow) / std_flow
                    if z_score > flow_threshold:
                        flagged_frames.append(i + 1)  # +1 because we start from frame 1
        
        # Normalize heatmap
        if np.max(heatmap_accumulator) > 0:
            heatmap_normalized = (heatmap_accumulator / np.max(heatmap_accumulator) * 255).astype(np.uint8)
        else:
            heatmap_normalized = np.zeros((height, width), dtype=np.uint8)
        
        # Apply colormap for visualization
        heatmap_colored = cv2.applyColorMap(heatmap_normalized, cv2.COLORMAP_JET)
        
        # Save heatmap as derivative artifact
        heatmap_path = os.path.join(
            os.path.dirname(video_path),
            f"optical_flow_{artifact.artifact_id}.png"
        )
        cv2.imwrite(heatmap_path, heatmap_colored)
        
        # Compute hash
        with open(heatmap_path, "rb") as f:
            heatmap_hash = hashlib.sha256(f.read()).hexdigest()
        
        # Create derivative artifact
        derivative_artifact = EvidenceArtifact.create_derivative(
            parent=artifact,
            artifact_type=ArtifactType.OPTICAL_FLOW_HEATMAP,
            file_path=heatmap_path,
            content_hash=heatmap_hash,
            action="optical_flow_analysis",
            agent_id="video_tools",
            metadata={
                "fps": fps,
                "frame_count": frame_count,
                "resolution": [width, height],
            }
        )
        
        return {
            "anomaly_heatmap_artifact": derivative_artifact,
            "flagged_frames": flagged_frames,
            "motion_stats": {
                "mean_flow": float(np.mean(flow_magnitudes)) if flow_magnitudes else 0.0,
                "std_flow": float(np.std(flow_magnitudes)) if flow_magnitudes else 0.0,
                "max_flow": float(np.max(flow_magnitudes)) if flow_magnitudes else 0.0,
                "min_flow": float(np.min(flow_magnitudes)) if flow_magnitudes else 0.0,
            },
            "video_info": {
                "fps": fps,
                "frame_count": frame_count,
                "resolution": [width, height],
            },
        }
    
    except Exception as e:
        if isinstance(e, ToolUnavailableError):
            raise
        raise ToolUnavailableError(f"Optical flow analysis failed: {str(e)}")


async def frame_window_extract(
    artifact: EvidenceArtifact,
    start_frame: int,
    end_frame: int,
) -> dict[str, Any]:
    """
    Extract a window of frames from video.
    
    Extracts frame range using OpenCV and creates a derivative artifact.
    
    Args:
        artifact: The evidence artifact to process
        start_frame: Starting frame number (0-indexed)
        end_frame: Ending frame number (exclusive)
    
    Returns:
        Dictionary containing:
        - frames_artifact: Derivative artifact with extracted frames
        - frame_count: Number of frames extracted
        - output_path: Path to the extracted frames directory
    
    Raises:
        ToolUnavailableError: If file cannot be processed
    """
    try:
        video_path = artifact.file_path
        if not os.path.exists(video_path):
            raise ToolUnavailableError(f"File not found: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ToolUnavailableError(f"Cannot open video: {video_path}")
        
        fps = cap.get(cv2.CAP_PROP_FPS)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        # Validate frame range
        start_frame = max(0, start_frame)
        end_frame = min(total_frames, end_frame)
        
        if start_frame >= end_frame:
            raise ToolUnavailableError(f"Invalid frame range: {start_frame} >= {end_frame}")
        
        # Create output directory for frames
        output_dir = os.path.join(
            os.path.dirname(video_path),
            f"frames_{artifact.artifact_id}_{start_frame}_{end_frame}"
        )
        os.makedirs(output_dir, exist_ok=True)
        
        # Seek to start frame
        cap.set(cv2.CAP_PROP_POS_FRAMES, start_frame)
        
        # Extract frames
        frame_count = 0
        frame_hashes = []
        
        for frame_idx in range(start_frame, end_frame):
            ret, frame = cap.read()
            if not ret:
                break
            
            # Save frame
            frame_path = os.path.join(output_dir, f"frame_{frame_idx:06d}.png")
            cv2.imwrite(frame_path, frame)
            
            # Compute hash
            frame_hash = hashlib.sha256(frame.tobytes()).hexdigest()
            frame_hashes.append(frame_hash)
            
            frame_count += 1
        
        cap.release()
        
        # Compute combined hash for all frames
        combined_hash = hashlib.sha256("".join(frame_hashes).encode()).hexdigest()
        
        # Create derivative artifact
        derivative_artifact = EvidenceArtifact.create_derivative(
            parent=artifact,
            artifact_type=ArtifactType.VIDEO_FRAME_WINDOW,
            file_path=output_dir,
            content_hash=combined_hash,
            action="frame_window_extract",
            agent_id="video_tools",
            metadata={
                "start_frame": start_frame,
                "end_frame": end_frame,
                "frame_count": frame_count,
                "fps": fps,
            }
        )
        
        return {
            "frames_artifact": derivative_artifact,
            "frame_count": frame_count,
            "output_path": output_dir,
        }
    
    except Exception as e:
        if isinstance(e, ToolUnavailableError):
            raise
        raise ToolUnavailableError(f"Frame window extraction failed: {str(e)}")


async def frame_consistency_analyze(
    frames_artifact: EvidenceArtifact,
    histogram_threshold: float = 0.5,
    edge_threshold: float = 0.3,
) -> dict[str, Any]:
    """
    Analyze frame consistency for discontinuities.
    
    Computes histogram diff and edge map diff between consecutive frames
    to detect potential splicing or editing.
    
    Args:
        frames_artifact: Artifact containing extracted frames (directory)
        histogram_threshold: Threshold for histogram difference
        edge_threshold: Threshold for edge difference
    
    Returns:
        Dictionary containing:
        - inconsistencies: List of detected inconsistencies
        - classification_hint: Hint about possible manipulation type
    
    Raises:
        ToolUnavailableError: If frames cannot be processed
    """
    try:
        frames_path = frames_artifact.file_path
        
        if not os.path.isdir(frames_path):
            raise ToolUnavailableError(f"Frames path is not a directory: {frames_path}")
        
        # Get list of frame files
        frame_files = sorted([
            f for f in os.listdir(frames_path)
            if f.endswith(('.png', '.jpg', '.jpeg'))
        ])
        
        if len(frame_files) < 2:
            return {
                "inconsistencies": [],
                "classification_hint": "insufficient_frames",
                "message": "Need at least 2 frames for consistency analysis",
            }
        
        inconsistencies = []
        prev_frame = None
        prev_hist = None
        prev_edges = None
        
        for i, frame_file in enumerate(frame_files):
            frame_path = os.path.join(frames_path, frame_file)
            frame = cv2.imread(frame_path)
            
            if frame is None:
                continue
            
            # Convert to grayscale
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Compute histogram
            hist = cv2.calcHist([gray], [0], None, [256], [0, 256])
            hist = cv2.normalize(hist, hist).flatten()
            
            # Compute edges
            edges = cv2.Canny(gray, 50, 150)
            
            if prev_frame is not None:
                # Compute histogram difference
                hist_diff = cv2.compareHist(
                    prev_hist, hist, cv2.HISTCMP_BHATTACHARYYA
                )
                
                # Compute edge difference
                edge_diff = np.sum(np.abs(prev_edges.astype(float) - edges.astype(float))) / (edges.shape[0] * edges.shape[1])
                
                # Check for inconsistencies
                if hist_diff > histogram_threshold:
                    inconsistencies.append(FrameInconsistency(
                        frame_pair=(i - 1, i),
                        diff_score=float(hist_diff),
                        type="histogram_discontinuity",
                    ))
                
                if edge_diff > edge_threshold:
                    inconsistencies.append(FrameInconsistency(
                        frame_pair=(i - 1, i),
                        diff_score=float(edge_diff),
                        type="edge_discontinuity",
                    ))
            
            prev_frame = gray
            prev_hist = hist
            prev_edges = edges
        
        # Determine classification hint
        classification_hint = "natural"
        
        if len(inconsistencies) > 0:
            hist_issues = sum(1 for i in inconsistencies if i.type == "histogram_discontinuity")
            edge_issues = sum(1 for i in inconsistencies if i.type == "edge_discontinuity")
            
            if hist_issues > edge_issues:
                classification_hint = "possible_color_grading_change"
            elif edge_issues > hist_issues:
                classification_hint = "possible_splice"
            else:
                classification_hint = "possible_editing"
        
        return {
            "inconsistencies": [i.to_dict() for i in inconsistencies],
            "classification_hint": classification_hint,
            "statistics": {
                "total_frames": len(frame_files),
                "inconsistency_count": len(inconsistencies),
                "histogram_issues": sum(1 for i in inconsistencies if i.type == "histogram_discontinuity"),
                "edge_issues": sum(1 for i in inconsistencies if i.type == "edge_discontinuity"),
            },
        }
    
    except Exception as e:
        if isinstance(e, ToolUnavailableError):
            raise
        raise ToolUnavailableError(f"Frame consistency analysis failed: {str(e)}")


async def face_swap_detect(
    frames_artifact: EvidenceArtifact,
    confidence_threshold: float = 0.5,
) -> dict[str, Any]:
    """
    Detect face swap/deepfake in video frames.
    
    Detects faces per frame and runs frequency-domain analysis
    on face regions to detect GAN-generated faces.
    
    NOTE: This is a heuristic stub implementation. For production use,
    integrate a model trained on FaceForensics++ dataset for accurate
    deepfake detection. The current implementation uses basic frequency
    analysis which may have high false positive/negative rates.
    
    Args:
        frames_artifact: Artifact containing extracted frames (directory)
        confidence_threshold: Threshold for flagging deepfake
    
    Returns:
        Dictionary containing:
        - deepfake_suspected: Boolean indicating if deepfake detected
        - confidence: Confidence level (0.0 to 1.0)
        - flagged_frames: List of frame numbers with suspected faces
        - face_count: Total number of faces detected
    
    Raises:
        ToolUnavailableError: If frames cannot be processed
    """
    try:
        frames_path = frames_artifact.file_path
        
        if not os.path.isdir(frames_path):
            raise ToolUnavailableError(f"Frames path is not a directory: {frames_path}")
        
        # Get list of frame files
        frame_files = sorted([
            f for f in os.listdir(frames_path)
            if f.endswith(('.png', '.jpg', '.jpeg'))
        ])
        
        if len(frame_files) == 0:
            return {
                "deepfake_suspected": False,
                "confidence": 0.0,
                "flagged_frames": [],
                "face_count": 0,
                "message": "No frames found for analysis",
            }
        
        # Load OpenCV's pre-trained face detector
        face_cascade = cv2.CascadeClassifier(
            cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'
        )
        
        flagged_frames = []
        total_faces = 0
        deepfake_scores = []
        
        for frame_idx, frame_file in enumerate(frame_files):
            frame_path = os.path.join(frames_path, frame_file)
            frame = cv2.imread(frame_path)
            
            if frame is None:
                continue
            
            # Convert to grayscale for face detection
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            
            # Detect faces
            faces = face_cascade.detectMultiScale(
                gray,
                scaleFactor=1.1,
                minNeighbors=5,
                minSize=(30, 30)
            )
            
            total_faces += len(faces)
            
            for (x, y, w, h) in faces:
                # Extract face region
                face_region = frame[y:y+h, x:x+w]
                
                if face_region.size == 0:
                    continue
                
                # Convert to grayscale
                face_gray = cv2.cvtColor(face_region, cv2.COLOR_BGR2GRAY)
                
                # Resize to standard size for analysis
                face_resized = cv2.resize(face_gray, (64, 64))
                
                # Apply FFT for frequency analysis
                # GAN-generated faces often have characteristic frequency patterns
                fft = np.fft.fft2(face_resized)
                fft_shift = np.fft.fftshift(fft)
                magnitude = np.abs(fft_shift)
                
                # Normalize magnitude
                magnitude_log = np.log1p(magnitude)
                magnitude_norm = (magnitude_log - np.min(magnitude_log)) / (
                    np.max(magnitude_log) - np.min(magnitude_log) + 1e-10
                )
                
                # Analyze frequency distribution
                # Real faces typically have more energy in low frequencies
                # GAN faces may have unusual high-frequency patterns
                
                center = np.array(magnitude_norm.shape) // 2
                y_coords, x_coords = np.ogrid[:magnitude_norm.shape[0], :magnitude_norm.shape[1]]
                distances = np.sqrt((x_coords - center[1])**2 + (y_coords - center[0])**2)
                
                # Compute energy in different frequency bands
                low_freq_mask = distances < 10
                high_freq_mask = distances > 20
                
                low_freq_energy = np.sum(magnitude_norm[low_freq_mask]**2)
                high_freq_energy = np.sum(magnitude_norm[high_freq_mask]**2)
                total_energy = low_freq_energy + high_freq_energy + 1e-10
                
                high_freq_ratio = high_freq_energy / total_energy
                
                # GAN faces often have higher high-frequency content
                # This is a heuristic and may not be accurate
                if high_freq_ratio > 0.4:
                    deepfake_scores.append(high_freq_ratio)
                    if frame_idx not in flagged_frames:
                        flagged_frames.append(frame_idx)
        
        # Compute overall confidence
        if len(deepfake_scores) > 0:
            mean_score = np.mean(deepfake_scores)
            confidence = min(1.0, mean_score)
        else:
            confidence = 0.0
        
        deepfake_suspected = confidence > confidence_threshold and len(flagged_frames) > 0
        
        return {
            "deepfake_suspected": deepfake_suspected,
            "confidence": confidence,
            "flagged_frames": flagged_frames,
            "face_count": total_faces,
            "analysis_method": "heuristic_frequency_analysis",
            "production_note": (
                "This is a heuristic stub implementation. For production use, "
                "integrate a model trained on FaceForensics++ dataset for "
                "accurate deepfake detection."
            ),
        }
    
    except Exception as e:
        if isinstance(e, ToolUnavailableError):
            raise
        raise ToolUnavailableError(f"Face swap detection failed: {str(e)}")


async def video_metadata_extract(
    artifact: EvidenceArtifact,
) -> dict[str, Any]:
    """
    Extract metadata from video file.
    
    Args:
        artifact: The evidence artifact to analyze
    
    Returns:
        Dictionary containing video metadata
    """
    try:
        video_path = artifact.file_path
        if not os.path.exists(video_path):
            raise ToolUnavailableError(f"File not found: {video_path}")
        
        # Open video
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            raise ToolUnavailableError(f"Cannot open video: {video_path}")
        
        # Extract metadata
        metadata = {
            "fps": cap.get(cv2.CAP_PROP_FPS),
            "frame_count": int(cap.get(cv2.CAP_PROP_FRAME_COUNT)),
            "width": int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)),
            "height": int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)),
            "fourcc": int(cap.get(cv2.CAP_PROP_FOURCC)),
            "backend": cap.getBackendName(),
        }
        
        # Convert fourcc to string
        fourcc = int(metadata["fourcc"])
        metadata["fourcc_str"] = "".join([chr((fourcc >> 8 * i) & 0xFF) for i in range(4)])
        
        # Compute duration
        if metadata["fps"] > 0:
            metadata["duration"] = metadata["frame_count"] / metadata["fps"]
        else:
            metadata["duration"] = 0
        
        cap.release()
        
        # Get file size
        metadata["file_size"] = os.path.getsize(video_path)
        
        return {
            "metadata": metadata,
            "file_path": video_path,
        }
    
    except Exception as e:
        if isinstance(e, ToolUnavailableError):
            raise
        raise ToolUnavailableError(f"Video metadata extraction failed: {str(e)}")
